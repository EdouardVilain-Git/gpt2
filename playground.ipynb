{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "645f7121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "57f9e18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from src.model import GPTConfig, GPT\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "db170ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_return_sequences = 5\n",
    "max_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4b8881af",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "de912162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "mps:0\n"
     ]
    }
   ],
   "source": [
    "model = GPT.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "model.to(device)\n",
    "print(next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b50e6a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps:0\n"
     ]
    }
   ],
   "source": [
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "tokens = enc.encode(\"Hello, I'm a language model,\")\n",
    "\n",
    "tokens = torch.tensor(tokens)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_sequences,1)\n",
    "tokens = tokens.to(device)\n",
    "print(tokens.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bb57eead",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "x = tokens\n",
    "while x.size(1) < max_length:\n",
    "    with torch.no_grad():\n",
    "        # Sample next token\n",
    "        logits = model(x)[:,-1,:]\n",
    "\n",
    "        # Only keep top k tokens\n",
    "        topk_logits, topk_indices = torch.topk(logits, 50, dim=-1)\n",
    "\n",
    "        # Compute probs\n",
    "        topk_probs = F.softmax(topk_logits, dim=-1)\n",
    "\n",
    "        # Sample best indices\n",
    "        ix = torch.multinomial(topk_probs, 1)\n",
    "\n",
    "        # Get generated tokens\n",
    "        xcol = torch.gather(topk_indices, -1, ix)\n",
    "\n",
    "        # Concatenate on the last dim\n",
    "        x = torch.concat((x, xcol), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d597e694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I'm a language model, that's very, where I have, my, \"I, I can be all like for me, my\n",
      "Hello, I'm a language model, and in the game, the games, the are actually a part of the whole, is that.\n",
      "\n",
      "\n",
      "\n",
      "Hello, I'm a language model, I'm a person, a human being, being a human being a human being, being human being be being\n",
      "Hello, I'm a language model, and I'm so and I'm so and I'm and I'm I and I am I'm I'm\n",
      "Hello, I'm a language model, and I'm a natural language language and, I'm, I, I'm a human person. I'm\n"
     ]
    }
   ],
   "source": [
    "# Print the generated text\n",
    "for i in range(num_return_sequences):\n",
    "    tokens = x[i, :max_length].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85a40d0",
   "metadata": {},
   "source": [
    "# Get Shakespear Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c4355adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-10-03 08:51:08--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Résolution de raw.githubusercontent.com (raw.githubusercontent.com)… 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connexion à raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443… connecté.\n",
      "requête HTTP transmise, en attente de la réponse… 200 OK\n",
      "Taille : 1115394 (1,1M) [text/plain]\n",
      "Sauvegarde en : « input.txt »\n",
      "\n",
      "input.txt           100%[===================>]   1,06M  --.-KB/s    ds 0,06s   \n",
      "\n",
      "2025-10-03 08:51:09 (17,3 MB/s) — « input.txt » sauvegardé [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f6597d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text=f.read()\n",
    "data = text[:1000]\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c793815e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = enc.encode(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c790c81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5962, 22307,    25,   198,  8421,   356,  5120,   597,  2252,    11,\n",
      "          3285,   502,  2740,    13,   198,   198],\n",
      "        [ 3237,    25,   198,  5248,   461,    11,  2740,    13,   198,   198,\n",
      "          5962, 22307,    25,   198,  1639,   389],\n",
      "        [  477, 12939,  2138,   284,  4656,   621,   284,  1145,   680,    30,\n",
      "           198,   198,  3237,    25,   198,  4965],\n",
      "        [ 5634,    13, 12939,    13,   198,   198,  5962, 22307,    25,   198,\n",
      "          5962,    11,   345,   760,   327,  1872],\n",
      "        [  385,  1526, 28599,   318,  4039,  4472,   284,   262,   661,    13,\n",
      "           198,   198,  3237,    25,   198,  1135],\n",
      "        [  760,   470,    11,   356,   760,   470,    13,   198,   198,  5962,\n",
      "         22307,    25,   198,  5756,   514,  1494],\n",
      "        [  683,    11,   290,   356,  1183,   423, 11676,   379,   674,   898,\n",
      "          2756,    13,   198,  3792,   470,   257],\n",
      "        [15593,    30,   198,   198,  3237,    25,   198,  2949,   517,  3375,\n",
      "           319,   470,    26,  1309,   340,   307]])\n",
      "tensor([[22307,    25,   198,  8421,   356,  5120,   597,  2252,    11,  3285,\n",
      "           502,  2740,    13,   198,   198,  3237],\n",
      "        [   25,   198,  5248,   461,    11,  2740,    13,   198,   198,  5962,\n",
      "         22307,    25,   198,  1639,   389,   477],\n",
      "        [12939,  2138,   284,  4656,   621,   284,  1145,   680,    30,   198,\n",
      "           198,  3237,    25,   198,  4965,  5634],\n",
      "        [   13, 12939,    13,   198,   198,  5962, 22307,    25,   198,  5962,\n",
      "            11,   345,   760,   327,  1872,   385],\n",
      "        [ 1526, 28599,   318,  4039,  4472,   284,   262,   661,    13,   198,\n",
      "           198,  3237,    25,   198,  1135,   760],\n",
      "        [  470,    11,   356,   760,   470,    13,   198,   198,  5962, 22307,\n",
      "            25,   198,  5756,   514,  1494,   683],\n",
      "        [   11,   290,   356,  1183,   423, 11676,   379,   674,   898,  2756,\n",
      "            13,   198,  3792,   470,   257, 15593],\n",
      "        [   30,   198,   198,  3237,    25,   198,  2949,   517,  3375,   319,\n",
      "           470,    26,  1309,   340,   307,  1760]])\n"
     ]
    }
   ],
   "source": [
    "buf = torch.tensor(tokens[:128+1])\n",
    "x = buf[:-1].view(8,16)\n",
    "y = buf[1:].view(8,16)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ae1a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
